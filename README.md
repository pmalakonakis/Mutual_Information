# Mutual_Information


Mutual information I(X;Y) computes the amount of information a random variable includes about another random variable, or in terms of entropy it is the decrease of uncertainty in a random variable due to existing knowledge about the other. More formally, for a pair of discrete random variables X, Y with joint probability function p(x,y) and marginal probability functions p(x) and p(y) respectively, the mutual information I(X;Y) is the relative entropy between the joint distribution and the product distribution:

<img src=https://raw.githubusercontent.com/pmalakonakis/Mutual_Information/master/mutualinformation.png>
